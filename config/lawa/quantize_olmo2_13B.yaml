awq: false
batch_size: 1
block_name_to_quantize: model.layers
block_size: 2048
bnb: false
dataset_path: /fast/atatjer/hf_fast/datasets/refinedweb/
debug: false
device: cuda
do_fuse: false
eval: true
fuse_max_seq_len: 512
gptq: true
gptq_dataset: c4
group_size: 128
calibration_batch_size: 16
gptq_model_seqlength: 512
hf_hub_name: scalinglawsquantized
model_file_patterns: '*.safetensors,*config.json'
model_quantized_dir: /fast/atatjer/scalinglawsquantization/lawa/
num_workers: 4
push_to_hf_hub: false
q_bits:
- 3
- 4
quantize: true
max_model_len: 4096

repo_origin:
- allenai/OLMo-2-1124-13B
revision:
- - 'stage2-ingredient1-step6000-tokens51B'
  - 'stage2-ingredient2-step6000-tokens51B'
  - 'stage2-ingredient3-step6000-tokens51B'
  - 'stage2-ingredient4-step11000-tokens93B'
  - 'stage2-ingredient4-step21000-tokens177B'
  - 'stage2-ingredient4-step31000-tokens261B'
  - 'stage2-ingredient1-step11931-tokens100B'
  - 'stage2-ingredient2-step11931-tokens100B'
  - 'stage2-ingredient3-step11931-tokens100B'
  - 'stage2-ingredient4-step35773-tokens300B'
  - 'main'
- - 'stage2-ingredient1-step6000-tokens51B'
  - 'stage2-ingredient2-step6000-tokens51B'
  - 'stage2-ingredient3-step6000-tokens51B'
  - 'stage2-ingredient4-step11000-tokens93B'
  - 'stage2-ingredient4-step21000-tokens177B'
  - 'stage2-ingredient4-step31000-tokens261B'
  - 'stage2-ingredient1-step11931-tokens100B'
  - 'stage2-ingredient2-step11931-tokens100B'
  - 'stage2-ingredient3-step11931-tokens100B'
  - 'stage2-ingredient4-step35773-tokens300B'
L:
- 4
- 6
- 10
save_quantized_model: True
tokenizer_name:
- allenai/OLMo-2-1124-13B
use_quant_cache: True
wandb_dir: /fast/atatjer/wandb/scalinglawsquantization
wandb_project: trainingdynamicsquantization
wandb_run_name: 13B-LAWA