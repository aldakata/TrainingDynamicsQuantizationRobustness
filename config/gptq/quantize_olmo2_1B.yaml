awq: false
batch_size: 1
block_name_to_quantize: model.layers
block_size: 2048
bnb: false
dataset_path: /fast/atatjer/hf_fast/datasets/refinedweb/
debug: false
device: cuda
do_fuse: false
eval: true
fuse_max_seq_len: 512
gptq: true
gptq_dataset: c4
group_size: 128
calibration_batch_size: 16
gptq_model_seqlength: 512
hf_hub_name: scalinglawsquantized
model_file_patterns: '*.safetensors,*config.json'
model_quantized_dir: /fast/atatjer/scalinglawsquantization/lawa/
num_workers: 4
push_to_hf_hub: false
q_bits:
- 4
- 3
quantize: True
max_model_len: 4096

repo_origin:
- allenai/OLMo-2-0425-1B
revision:
-  stage1-step40000-tokens84B
-  stage1-step90000-tokens189B
-  stage1-step120000-tokens252B
-  stage1-step180000-tokens378B
-  stage1-step210000-tokens441B
-  stage1-step270000-tokens567B
-  stage1-step330000-tokens693B
-  stage1-step360000-tokens755B
-  stage1-step420000-tokens881B
-  stage1-step450000-tokens944B
-  stage1-step510000-tokens1070B
-  stage1-step570000-tokens1196B
-  stage1-step600000-tokens1259B
-  stage1-step660000-tokens1385B
-  stage1-step690000-tokens1448B
-  stage1-step750000-tokens1573B
-  stage1-step810000-tokens1699B
-  stage1-step840000-tokens1762B
-  stage1-step900000-tokens1888B
-  stage1-step930000-tokens1951B
-  stage1-step960000-tokens2014B
-  stage1-step1020000-tokens2140B
-  stage1-step1050000-tokens2203B
-  stage1-step1110000-tokens2328B
-  stage1-step1170000-tokens2454B
-  stage1-step1200000-tokens2517B
-  stage1-step1260000-tokens2643B
-  stage1-step1320000-tokens2769B
-  stage1-step1380000-tokens2895B
-  stage1-step1410000-tokens2957B
-  stage1-step1470000-tokens3083B
-  stage1-step1500000-tokens3146B
-  stage1-step1560000-tokens3272B
-  stage1-step1590000-tokens3335B
-  stage1-step1650000-tokens3461B
-  stage1-step1680000-tokens3524B
-  stage1-step1740000-tokens3650B
-  stage1-step1770000-tokens3712B
-  stage1-step1830000-tokens3838B
-  stage1-step1890000-tokens3964B
-  stage2-ingredient1-step7000-tokens15B
-  stage2-ingredient1-step19000-tokens40B
-  stage2-ingredient1-step23852-tokens51B
-  stage2-ingredient2-step7000-tokens15B
-  stage2-ingredient2-step19000-tokens40B
-  stage2-ingredient2-step23852-tokens51B
-  stage2-ingredient3-step7000-tokens15B
-  stage2-ingredient3-step19000-tokens40B
-  stage2-ingredient3-step23852-tokens51B
-  main

save_quantized_model: True
tokenizer_name:
- allenai/OLMo-2-0425-1B
use_quant_cache: False
wandb_dir: /fast/atatjer/wandb/scalinglawsquantization
wandb_project: trainingdynamicsquantization
wandb_run_name: allenai/OLMo-2-0425-1B