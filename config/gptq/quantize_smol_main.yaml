## CONFIG file
# debug: True
debug: False

# Cluster utils
revision:
  [
  'main'
  ]

save_quantized_model: True
max_model_len: 4096
# Eval
eval: True
quantize: True
repo_origin: "HuggingFaceTB/SmolLM3-3B"
tokenizer_name: HuggingFaceTB/SmolLM3-3B
block_size: 2048
batch_size: 1
num_workers: 4
device: "cuda"

dataset_path: "/fast/atatjer/hf_fast/datasets/refinedweb/" # 600k tkns
model_quantized_dir: /fast/atatjer/scalinglawsquantization/modelsquantized
model_file_patterns: ""

# GPTQ
gptq: True
gptq_dataset: "c4"
group_size: 128
calibration_batch_size: 16

# AWQ
awq: False
do_fuse: False
fuse_max_seq_len: 512

bnb: False

# Quantization
gptq_model_seqlength: 512
q_bits:
  - 3
  - 4
block_name_to_quantize: model.layers
use_quant_cache: True

# HF
push_to_hf_hub: False
hf_hub_name: "scalinglawsquantized"

# WANDB
log_wandb: True
wandb_project: trainingdynamicsquantization
wandb_run_name: smol-main
wandb_dir: /fast/atatjer/wandb/scalinglawsquantization
