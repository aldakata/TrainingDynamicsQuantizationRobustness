## CONFIG file
# debug: True
debug: False

# Cluster utils
revision:
  [
    "stage1-step-40000",
    "stage1-step-80000",
    "stage1-step-120000",
    "stage1-step-160000",
    "stage3-step-4240000",
    "stage3-step-4400000",
    "stage3-step-4600000",
    "stage3-step-4720000",
    "stage2-step-3480000",
    "stage2-step-3680000",
    "stage2-step-3880000",
    "stage2-step-4080000",
    "stage2-step-4200000",
    "stage1-step-200000",
    "stage1-step-400000",
    "stage1-step-600000",
    "stage1-step-800000",
    "stage1-step-1000000",
    "stage1-step-1200000",
    "stage1-step-1400000",
    "stage1-step-1600000",
    "stage1-step-1800000",
    "stage1-step-2000000",
    "stage1-step-2200000",
    "stage1-step-2400000",
    "stage1-step-2600000",
    "stage1-step-2800000",
    "stage1-step-3000000",
    "stage1-step-3200000",
    "stage1-step-3400000",
    "lc-32k-to-64k-step-20000",
    "lc-32k-to-64k-step-12000",
    "lc-32k-to-64k-step-4000",
    "lc-32k-to-64k-step-8000",
    "lc-4k-to-32k-step-12000",
    "lc-32k-to-64k-step-16000",
    "lc-4k-to-32k-step-4000",
    "lc-4k-to-32k-step-20000",
    "lc-4k-to-32k-step-8000",
    "lc-4k-to-32k-step-16000",
    "it-LC-expert",
    "it-soup-APO",
    "it-SFT",
    "it-mid-training",
    stage3-step-4360000,
    stage3-step-4440000,
    stage3-step-4480000,
    stage3-step-4520000,
    stage3-step-4560000,
    stage3-step-4640000,
    stage3-step-4680000,
  ]

save_quantized_model: True
max_model_len: 4096
# Eval
eval: True
quantize: True
repo_origin: "HuggingFaceTB/SmolLM3-3B-checkpoints"
tokenizer_name: HuggingFaceTB/SmolLM3-3B
block_size: 2048
batch_size: 1
num_workers: 4
device: "cuda"

dataset_path: "/fast/atatjer/hf_fast/datasets/refinedweb/" # 600k tkns
model_quantized_dir: /fast/atatjer/scalinglawsquantization/modelsquantized
model_file_patterns: ""

# GPTQ
gptq: True
gptq_dataset: "c4"
group_size: 128
calibration_batch_size: 16

# AWQ
awq: False
do_fuse: False
fuse_max_seq_len: 512

bnb: False

# Quantization
gptq_model_seqlength: 512
q_bits:
  - 3
  - 4
block_name_to_quantize: model.layers
use_quant_cache: True

# HF
push_to_hf_hub: False
hf_hub_name: "scalinglawsquantized"

# WANDB
log_wandb: True
wandb_project: trainingdynamicsquantization
wandb_run_name: smol
wandb_dir: /fast/atatjer/wandb/scalinglawsquantization
